{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全局参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#数据集目录\n",
    "BASE_PATH = '/Users/hydeli/workspace/private/keep-ml/homework-1/data/data1/20_newsgroups/'\n",
    "\n",
    "#分类文件夹\n",
    "#DATA_FOLDERS = ['alt.atheism', 'misc.forsale']\n",
    "DATA_FOLDERS = [name for name in os.listdir(BASE_PATH) if not os.path.isfile(BASE_PATH + name)]\n",
    "\n",
    "#训练数据集样本数\n",
    "TRAIN_DATA_LIMITATION = 500\n",
    "\n",
    "#测试数据集样本数\n",
    "TEST_DATA_LIMITATION = 100\n",
    "\n",
    "#平滑因子\n",
    "SMOOTHING = 0.0001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据\n",
    "\n",
    "把所有数据加载到一个两层的字典，第一层为 news_group_name，第二层为文件全路径和文件内容\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载样本文件 :\n",
      "从 [talk.politics.mideast] 导入 600 份样本\n",
      "从 [rec.autos] 导入 600 份样本\n",
      "从 [comp.sys.mac.hardware] 导入 600 份样本\n",
      "从 [alt.atheism] 导入 600 份样本\n",
      "从 [rec.sport.baseball] 导入 600 份样本\n",
      "从 [comp.os.ms-windows.misc] 导入 600 份样本\n",
      "从 [rec.sport.hockey] 导入 600 份样本\n",
      "从 [sci.crypt] 导入 600 份样本\n",
      "从 [sci.med] 导入 600 份样本\n",
      "从 [talk.politics.misc] 导入 600 份样本\n",
      "从 [rec.motorcycles] 导入 600 份样本\n",
      "从 [comp.windows.x] 导入 600 份样本\n",
      "从 [comp.graphics] 导入 600 份样本\n",
      "从 [comp.sys.ibm.pc.hardware] 导入 600 份样本\n",
      "从 [sci.electronics] 导入 600 份样本\n",
      "从 [talk.politics.guns] 导入 600 份样本\n",
      "从 [sci.space] 导入 600 份样本\n",
      "从 [soc.religion.christian] 导入 600 份样本\n",
      "从 [misc.forsale] 导入 600 份样本\n",
      "从 [talk.religion.misc] 导入 600 份样本\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def open_and_read(path):\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        data = file.read()\n",
    "        return data\n",
    "    \n",
    "def load_ori_data():\n",
    "    ori_datas = {}\n",
    "    print ('加载样本文件 :')\n",
    "    for folder_name in DATA_FOLDERS:\n",
    "        folder_path = BASE_PATH + folder_name + '/'\n",
    "        datas_in_group = {}\n",
    "        for idx, file_name in enumerate(name for name in os.listdir(folder_path) if os.path.isfile(folder_path + name)):\n",
    "            datas_in_group[file_name] = open_and_read(folder_path + file_name)\n",
    "            if idx == TRAIN_DATA_LIMITATION+TEST_DATA_LIMITATION-1:\n",
    "                break\n",
    "        ori_datas[folder_name] = datas_in_group\n",
    "        print ('从 [%s] 导入 %s 份样本' % (folder_name, len(datas_in_group)))\n",
    "    return ori_datas\n",
    "\n",
    "ori_datas = load_ori_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样本划分\n",
    "将数据随机划分为训练集和测试集\n",
    "每个数据集包含样本数由全局参数 TRAIN_DATA_LIMITATION 和 TEST_DATA_LIMITATION 定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "def random_split_data_set(ori_datas):\n",
    "    train_datas = {}\n",
    "    test_datas = {}\n",
    "    for (group, datas_in_group) in ori_datas.items():\n",
    "        copies = copy.deepcopy(datas_in_group)\n",
    "        \n",
    "        train_subset = {}\n",
    "        while len(train_subset) < TRAIN_DATA_LIMITATION:\n",
    "            doc_name, txt = random.choice(list(copies.items()))\n",
    "            train_subset[doc_name] = txt\n",
    "            copies.pop(doc_name)\n",
    "        train_datas[group] = train_subset\n",
    "        \n",
    "        test_subset = {}\n",
    "        while len(test_subset) < TEST_DATA_LIMITATION:\n",
    "            doc_name, txt = random.choice(list(copies.items()))\n",
    "            test_subset[doc_name] = txt\n",
    "            copies.pop(doc_name)\n",
    "        test_datas[group] = test_subset\n",
    "        \n",
    "    return train_datas, test_datas\n",
    "\n",
    "train_datas, test_datas = random_split_data_set(ori_datas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "去掉标点符号，转为小写，分词后提取词干，过滤停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def word_tokenize(txt):\n",
    "    #将标点符号转置为空格\n",
    "    translator = str.maketrans(string.punctuation, ' ' * (len(string.punctuation)))\n",
    "    txt = txt.translate(translator)\n",
    "    \n",
    "    #大写转小写\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    #分词\n",
    "    tokens = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "    sw_set = set(stopwords.words('english'))\n",
    "    tokenized = []\n",
    "    for token in tokens:\n",
    "        #词干提取\n",
    "        token = PorterStemmer().stem(token)\n",
    "        #过滤停用词\n",
    "        if token not in sw_set and not token.isdigit():\n",
    "            tokenized.append(token)\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_datas = {}\n",
    "for (group, datas_in_group) in train_datas.items():\n",
    "    tokenized_in_group = {}\n",
    "    for (file_name, txt) in datas_in_group.items():\n",
    "        tokenized_in_group[file_name] = word_tokenize(txt)\n",
    "    tokenized_datas[group] = tokenized_in_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 tf-idf\n",
    "\n",
    "tf：词频\n",
    "idf：逆文档频率\n",
    "tf-idf：tf * idf \n",
    "tf-idf 是重要性调整系数，衡量一个词是不是常见词。如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from math import log\n",
    "\n",
    "\n",
    "def freq(tokens):\n",
    "    return nltk.FreqDist(tokens)\n",
    "\n",
    "def cal_tf_idf(tokenized_datas):\n",
    "\n",
    "    #计算 tf 和 idf\n",
    "    tf = {}\n",
    "    idf = {}\n",
    "    freq_in_all={}\n",
    "    file_count = 0\n",
    "    for (group, datas_in_group) in tokenized_datas.items():\n",
    "        freq_in_group = nltk.FreqDist()\n",
    "        for (file_name, tokens) in datas_in_group.items():\n",
    "            freq_in_file = freq(tokens)\n",
    "            \n",
    "            # tf\n",
    "            freq_in_group = freq_in_group + freq_in_file\n",
    "            \n",
    "            #idf\n",
    "            file_count += 1\n",
    "            for token in freq_in_file.keys():\n",
    "               if freq_in_all.get(token) is None:\n",
    "                   freq_in_all[token] = 1\n",
    "               else:\n",
    "                   freq_in_all[token] += 1\n",
    "                    \n",
    "        tf[group] = freq_in_group\n",
    "        \n",
    "    for (token, count) in freq_in_all.items():\n",
    "        idf[token] = log(file_count/count+1)\n",
    "    \n",
    "    #计算tf-idf\n",
    "    for (group, tf_in_group) in tf.items():\n",
    "        for (token, count) in tf_in_group.items():\n",
    "            tf_in_group[token] = count * idf[token]\n",
    "    return tf\n",
    "\n",
    "tf_idf = cal_tf_idf(tokenized_datas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算朴素贝叶斯的 likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(new_tf, group_tf_idf):\n",
    "    word_count_in_group = group_tf_idf.N()\n",
    "    lh = 0\n",
    "    for token in new_tf.keys():\n",
    "        word_tf_idf = 0 if group_tf_idf.get(token) is None else group_tf_idf.get(token)\n",
    "        power = new_tf[token]\n",
    "        lh += log((word_tf_idf + SMOOTHING) / word_count_in_group) * power\n",
    "    return lh\n",
    "\n",
    "def max_likelihood(txt, tf_idf):\n",
    "    tp_list = []\n",
    "    new_tf = freq(word_tokenize(txt))\n",
    "    for (group, group_tf_idf) in tf_idf.items():\n",
    "        tp_list.append((group, likelihood(new_tf, tf_idf[group])))\n",
    "    max_tp = max(tp_list, key=lambda item: item[1])\n",
    "    return max_tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试\n",
    "验证对测试集的分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag: talk.politics.mideast, error: 5, total:100, error_rate:0.05\n",
      "tag: rec.autos, error: 4, total:100, error_rate:0.04\n",
      "tag: comp.sys.mac.hardware, error: 11, total:100, error_rate:0.11\n",
      "tag: alt.atheism, error: 15, total:100, error_rate:0.15\n",
      "tag: rec.sport.baseball, error: 5, total:100, error_rate:0.05\n",
      "tag: comp.os.ms-windows.misc, error: 44, total:100, error_rate:0.44\n",
      "tag: rec.sport.hockey, error: 5, total:100, error_rate:0.05\n",
      "tag: sci.crypt, error: 5, total:100, error_rate:0.05\n",
      "tag: sci.med, error: 3, total:100, error_rate:0.03\n",
      "tag: talk.politics.misc, error: 25, total:100, error_rate:0.25\n",
      "tag: rec.motorcycles, error: 7, total:100, error_rate:0.07\n",
      "tag: comp.windows.x, error: 14, total:100, error_rate:0.14\n",
      "tag: comp.graphics, error: 28, total:100, error_rate:0.28\n",
      "tag: comp.sys.ibm.pc.hardware, error: 21, total:100, error_rate:0.21\n",
      "tag: sci.electronics, error: 14, total:100, error_rate:0.14\n",
      "tag: talk.politics.guns, error: 9, total:100, error_rate:0.09\n",
      "tag: sci.space, error: 6, total:100, error_rate:0.06\n",
      "tag: soc.religion.christian, error: 1, total:100, error_rate:0.01\n",
      "tag: misc.forsale, error: 15, total:100, error_rate:0.15\n",
      "tag: talk.religion.misc, error: 39, total:100, error_rate:0.39\n",
      "error: 276, total:2000, error_rate:0.138\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def acc(test_datas, tf_idf):\n",
    "    total = 0\n",
    "    error = 0\n",
    "    tag_stat = []\n",
    "    for (group, datas_in_group) in test_datas.items():\n",
    "        tag_total = 0\n",
    "        tag_error = 0\n",
    "        for (key, txt) in datas_in_group.items():\n",
    "            max_tp = max_likelihood(txt, tf_idf)\n",
    "            total += 1\n",
    "            tag_total += 1\n",
    "            if (group != max_tp[0]):\n",
    "                error += 1\n",
    "                tag_error += 1\n",
    "        tag_stat.append((group, tag_error, tag_total, tag_error / tag_total))\n",
    "        print('tag: %s, error: %s, total:%s, error_rate:%s' % (group, tag_error, tag_total, tag_error / tag_total))\n",
    "    print('error: %s, total:%s, error_rate:%s' % (error, total, error / total))\n",
    "\n",
    "acc(test_datas, tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
